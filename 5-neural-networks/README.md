# **NEURAL NETWORKS** 

The lecture explores the mathematical concepts, and structure of a Neural Network, explaining various activation functions attached to a perceptron, as well as the significance of Weights and Biases on the prediction accuracy of a Neural Network. More importantly, the purpose of the Gradient Descent Algorithm and the role it plays in calibrating a Neural Network. Backpropagation is also covered as well as techniques to combat overfitting, such as Dropout. 

The lecture also introduces `TensorFlow` - a very famous Deep Learning library, and provides an introduction, into Computer Vision, and the architecture of a Convolutional Neural Network (CNN) for image recognition, which proves to be of great importance today. 

Finally, Recurrent Neural Networks (RNNs) are introduced to deal with cases where the network receives sequences, and not a single object. 